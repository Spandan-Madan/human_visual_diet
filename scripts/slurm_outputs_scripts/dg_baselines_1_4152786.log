1
Environment:
	Python: 3.8.11
	PyTorch: 1.9.1+cu102
	Torchvision: 0.10.1+cu102
	CUDA: 10.2
	CUDNN: 7605
	NumPy: 1.21.2
	PIL: 8.3.1
Args:
	algorithm: IGA
	checkpoint_freq: None
	data_dir: ./domainbed/data/
	dataset: DGMaterials
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [6]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 8
	class_balanced: False
	data_augmentation: True
	lr: 5e-05
	nonlinear_classifier: False
	penalty: 1000
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0
env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_in_acc   env4_out_acc  env5_in_acc   env5_out_acc  env6_in_acc   env6_out_acc  epoch         loss          mem_gb        penalty       step          step_time    
0.0363506261  0.0387863174  0.0392486583  0.0380707027  0.0389982111  0.0385716330  0.0392844365  0.0382138257  0.0379606440  0.0364247889  0.0421824687  0.0420065837  0.0391592129  0.0387863174  0.0000000000  2.7111794949  9.0359272957  874.58197021  0             4.1734437943 
0.1424150268  0.1451266638  0.1450626118  0.1436954344  0.1423971377  0.1439101188  0.1429516995  0.1454129097  0.1433989267  0.1441248032  0.1439355993  0.1459854015  0.1450268336  0.1466294547  0.0429338104  2.5663734500  9.3269133568  4.6945829431  300           2.7581099772 
0.1448479428  0.1445541720  0.1457781753  0.1451982253  0.1450089445  0.1468441391  0.1447942755  0.1474166309  0.1450447227  0.1470588235  0.1455456172  0.1467010162  0.1453488372  0.1471303850  0.0858676208  2.5646701201  9.3269133568  1.1113897707  600           2.7360884468 
0.1459749553  0.1462716473  0.1461896243  0.1454129097  0.1455277281  0.1480606841  0.1453309481  0.1484184915  0.1456887299  0.1475597538  0.1458318426  0.1469157006  0.1456350626  0.1479891227  0.1288014311  2.5675164708  9.3269133568  1.5111516279  900           2.8577603229 
0.2850089445  0.2843137255  0.2846869410  0.2855302705  0.2854919499  0.2823815658  0.2846869410  0.2856018320  0.2841502683  0.2875339917  0.2854382826  0.2823815658  0.2855992844  0.2814512666  0.1717352415  2.5661771560  9.3269133568  1.7935915909  1200          2.7445324095 
0.1459749553  0.1462716473  0.1461896243  0.1454129097  0.1455456172  0.1479891227  0.1454382826  0.1484184915  0.1456529517  0.1475597538  0.1458139535  0.1469157006  0.1456350626  0.1478459997  0.2146690519  2.5628296264  9.3269133568  1.5399858586  1500          2.7721533418 
0.0555635063  0.0568913697  0.0566905188  0.0523829970  0.0550447227  0.0588235294  0.0561538462  0.0546729641  0.0553667263  0.0576069844  0.0552415027  0.0583225991  0.0556171735  0.0573207385  0.2576028623  2.5694666608  9.3269133568  1.4618253207  1800          2.7874386017 
0.1647048301  0.1670244740  0.1650626118  0.1655932446  0.1652415027  0.1648776299  0.1650268336  0.1657363675  0.1648479428  0.1664519823  0.1651162791  0.1653785602  0.1657423971  0.1628739087  0.3005366726  2.5606213148  9.3269133568  1.2352760148  2100          2.7464319539 
0.1459749553  0.1462716473  0.1461896243  0.1454129097  0.1455456172  0.1479891227  0.1454382826  0.1484184915  0.1456529517  0.1475597538  0.1458139535  0.1469157006  0.1455992844  0.1477744382  0.3434704830  2.5678706447  9.3269133568  1.0831537737  2400          2.7602569803 
0.1647048301  0.1670244740  0.1650626118  0.1655932446  0.1652415027  0.1648776299  0.1650268336  0.1657363675  0.1648479428  0.1664519823  0.1651162791  0.1653785602  0.1657423971  0.1628739087  0.3864042934  2.5622944999  9.3269133568  0.9535575871  2700          2.7491759268 
0.1459749553  0.1462716473  0.1461896243  0.1454129097  0.1455456172  0.1479891227  0.1454382826  0.1484184915  0.1456529517  0.1475597538  0.1458139535  0.1469157006  0.1455992844  0.1477744382  0.4293381038  2.5646132398  9.3269133568  0.7848505085  3000          2.7453106380 
0.1459749553  0.1462716473  0.1461896243  0.1454129097  0.1455456172  0.1479891227  0.1454382826  0.1484184915  0.1456529517  0.1475597538  0.1458139535  0.1469157006  0.1455992844  0.1477744382  0.4722719141  2.5651478195  9.3269133568  0.7140565552  3300          2.7547535706 
0.1459749553  0.1462716473  0.1461896243  0.1454129097  0.1455456172  0.1479891227  0.1454382826  0.1484184915  0.1456529517  0.1475597538  0.1458139535  0.1469157006  0.1455992844  0.1477744382  0.5152057245  2.5636074511  9.3269133568  0.6742960122  3600          2.9300048963 
0.1459749553  0.1462716473  0.1461896243  0.1454129097  0.1455456172  0.1479891227  0.1454382826  0.1484184915  0.1456529517  0.1475597538  0.1458139535  0.1469157006  0.1455992844  0.1477744382  0.5581395349  2.5632509899  9.3269133568  0.6871568107  3900          2.7539597416 
0.1459749553  0.1462716473  0.1461896243  0.1454129097  0.1455456172  0.1479891227  0.1454382826  0.1484184915  0.1456529517  0.1475597538  0.1458139535  0.1469157006  0.1455992844  0.1477744382  0.6010733453  2.5637887502  9.3269133568  0.6429973072  4200          2.7453550959 
0.1459749553  0.1462716473  0.1461896243  0.1454129097  0.1455456172  0.1479891227  0.1454382826  0.1484184915  0.1456529517  0.1475597538  0.1458139535  0.1469157006  0.1455992844  0.1477744382  0.6440071556  2.5641957458  9.3269133568  0.6082922977  4500          2.7602233601 
0.1459749553  0.1462716473  0.1461896243  0.1454129097  0.1455456172  0.1479891227  0.1454382826  0.1484184915  0.1456529517  0.1475597538  0.1458139535  0.1469157006  0.1455992844  0.1477744382  0.6869409660  2.5638798912  9.3269133568  0.6062800531  4800          2.8038296541 
0.1459749553  0.1462716473  0.1461896243  0.1454129097  0.1455456172  0.1479891227  0.1454382826  0.1484184915  0.1456529517  0.1475597538  0.1458139535  0.1469157006  0.1455992844  0.1477744382  0.7155635063  2.5634946454  9.3269133568  0.5743331814  5000          2.7704146159 
