37
2022-05-22 22:51:51.880498: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2022-05-22 22:51:51.880735: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
Task is classification, not lift the flap.
Hard mining set to:both
Using materials: ['main_xml_50_2', 'main_xml_50_3', 'main_xml_50_4', 'main_xml_50_5']
Hotswapped material loader being used.
{1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 5, 7: 6, 8: 7, 9: 8, 10: 9, 11: 10, 12: 11, 13: 12}
-------------------------------
Annotation Counts
-------------------------------
desk                       4829
trash_bin                  3204
chair                     19905
window                    10204
lamp                       2489
table                     11541
monitor                    3094
file_cabinet               1727
bookshelf                  3902
bed                        3214
sofa                       3550
bathtub                    1988
flowerpot                   227
Total                     69874
-------------------------------

annotations: ../openrooms/annotation_files/train_main_xml_50_v3.json
batch_size: 20
checkpoint: null
git: e22ff54d4e15e859ff3672c5fc808cec0abfc80d
imagedir: ../
imbalance_reweighting: false
learning_rate: 1.0e-05
num_classes: 13
num_decoder_heads: 8
num_decoder_layers: 6
test_annotations: null
test_imagedir: null
uncertainty_gate_type: learned
uncertainty_threshold: 0.8
weighted_prediction: false

using non pretrained model
No checkpoint was passed.
Starting epoch: 1
batch num:0
overall: tensor(6.2760, device='cuda:0', grad_fn=<AddBackward0>)
Traceback (most recent call last):
  File "train_hotswapped_from_scratch.py", line 214, in <module>
    output_uncertainty_branch , output_main_branch, output_weighted, uncertainty = model(context_images_all, target_images_all, bbox_all)
  File "/n/home05/smadan/.conda/envs/domain_adaptation/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/net/coxfs01/srv/export/coxfs01/pfister_lab2/share_root/Lab/spandan/contextual_domain_adaptation/WhenPigsFlyContext/core/model_non_pretrained.py", line 99, in forward
    context_encoding = self.context_encoder(context_images)
  File "/n/home05/smadan/.conda/envs/domain_adaptation/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/net/coxfs01/srv/export/coxfs01/pfister_lab2/share_root/Lab/spandan/contextual_domain_adaptation/WhenPigsFlyContext/core/model_non_pretrained.py", line 156, in forward
    return self.encoder(image)
  File "/n/home05/smadan/.conda/envs/domain_adaptation/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/n/home05/smadan/.conda/envs/domain_adaptation/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/n/home05/smadan/.conda/envs/domain_adaptation/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/n/home05/smadan/.conda/envs/domain_adaptation/lib/python3.8/site-packages/torchvision/models/densenet.py", line 124, in forward
    new_features = layer(features)
  File "/n/home05/smadan/.conda/envs/domain_adaptation/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/n/home05/smadan/.conda/envs/domain_adaptation/lib/python3.8/site-packages/torchvision/models/densenet.py", line 90, in forward
    bottleneck_output = self.bn_function(prev_features)
  File "/n/home05/smadan/.conda/envs/domain_adaptation/lib/python3.8/site-packages/torchvision/models/densenet.py", line 50, in bn_function
    concated_features = torch.cat(inputs, 1)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 15.78 GiB total capacity; 13.85 GiB already allocated; 19.75 MiB free; 14.53 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
