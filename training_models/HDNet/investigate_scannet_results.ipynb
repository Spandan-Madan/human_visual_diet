{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import pathlib\n",
    "import json\n",
    "import yaml\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "from ml_collections import ConfigDict\n",
    "\n",
    "from core.dataset import COCODatasetWithID\n",
    "from core.config import save_config\n",
    "from core.model import Model\n",
    "from core.metrics import AccuracyLogger, IndividualScoreLogger\n",
    "import torchvision\n",
    "import pickle\n",
    "import timm\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_dir = 'v3_output/non_scannet_paired_augmented/'\n",
    "# model_name = out_dir.split('/')[-2].split('_')[0]\n",
    "\n",
    "# config_file = os.path.join(out_dir, 'config.yaml')\n",
    "\n",
    "# config_file\n",
    "\n",
    "# os.listdir(out_dir)\n",
    "\n",
    "# checkpoint_file = os.path.join(out_dir, 'checkpoint_5.tar')\n",
    "\n",
    "# with open(config_file) as f:\n",
    "#     cfg = ConfigDict(yaml.load(f, Loader=yaml.Loader))\n",
    "\n",
    "# checkpoint = torch.load(checkpoint_file, map_location=\"cpu\")\n",
    "\n",
    "# NUM_CLASSES=13\n",
    "\n",
    "# cfg.pretrained = False\n",
    "\n",
    "# if model_name == 'resnet':\n",
    "#     # model_ft = models.resnet18(pretrained=False, num_classes = NUM_CLASSES)\n",
    "#     model = models.resnet18(pretrained=False)\n",
    "#     model.fc = nn.Linear(512, NUM_CLASSES)\n",
    "#     simple_model = True\n",
    "# elif model_name == 'squeezenet':\n",
    "#     # model_ft = models.squeezenet1_1(pretrained=False, num_classes = NUM_CLASSES)\n",
    "#     model = models.squeezenet1_1(pretrained=False)\n",
    "#     model.classifier[1] = nn.Conv2d(512, NUM_CLASSES, kernel_size=(1,1), stride=(1,1))\n",
    "#     simple_model = True\n",
    "# elif model_name == 'densenet':\n",
    "#     # model_ft = models.densenet121(pretrained=False, num_classes = NUM_CLASSES)\n",
    "#     model = models.densenet121(pretrained=False)\n",
    "#     model.classifier = nn.Linear(1024, NUM_CLASSES)\n",
    "#     simple_model = True\n",
    "# elif model_name == 'mobilenet':\n",
    "#     # model_ft = models.mobilenet_v2(pretrained=False, num_classes = NUM_CLASSES)\n",
    "#     model = models.mobilenet_v2(pretrained=False)\n",
    "#     model.classifier[1] = torch.nn.Linear(in_features=model.classifier[1].in_features, out_features=NUM_CLASSES)\n",
    "#     simple_model = True\n",
    "# elif model_name == 'vit':\n",
    "#     model = timm.create_model('vit_base_patch16_224', pretrained=False)\n",
    "#     model.head = nn.Linear(768, NUM_CLASSES)\n",
    "#     simple_model = True\n",
    "# else:\n",
    "#     model = Model.from_config(cfg)\n",
    "#     simple_model = False\n",
    "# missing_keys, unexpected_keys = model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
    "# assert not missing_keys, \"Checkpoint is missing keys required to initialize the model: {}\".format(missing_keys)\n",
    "# if len(unexpected_keys):\n",
    "#     print(\"Checkpoint contains unexpected keys that were not used to initialize the model: \")\n",
    "#     print(unexpected_keys)\n",
    "\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device);\n",
    "\n",
    "# annotations_file = '../openrooms/annotation_files/scannet_handmade_test_set.json'\n",
    "\n",
    "# imagedir = '../'\n",
    "\n",
    "# testset = COCODatasetWithID(annotations_file, imagedir, (224,224), normalize_means=[0.485, 0.456, 0.406], normalize_stds=[0.229, 0.224, 0.225])\n",
    "# dataloader = DataLoader(testset, batch_size=100, num_workers=1, shuffle=False, drop_last=False)\n",
    "\n",
    "# class UnNormalize(object):\n",
    "#     def __init__(self, mean, std):\n",
    "#         self.mean = mean\n",
    "#         self.std = std\n",
    "\n",
    "#     def __call__(self, tensor):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
    "#         Returns:\n",
    "#             Tensor: Normalized image.\n",
    "#         \"\"\"\n",
    "#         for t, m, s in zip(tensor, self.mean, self.std):\n",
    "#             t.mul_(s).add_(m)\n",
    "#             # The normalize code -> t.sub_(m).div_(s)\n",
    "#         return tensor\n",
    "\n",
    "# unorm = UnNormalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "\n",
    "# %matplotlib inline\n",
    "\n",
    "# testset.idx2label\n",
    "\n",
    "# def morph_labels(vec):\n",
    "#     for i in range(len(vec)):\n",
    "#         if vec[i].item() == 1:\n",
    "#             vec[i] = 0\n",
    "#         if vec[i].item() == 6:\n",
    "#             vec[i] = 3\n",
    "#     return vec\n",
    "\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# corrects = 0\n",
    "# totals = 0\n",
    "# model.eval() # set eval mode\n",
    "# all_matches = []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for i, (context_images, target_images, bbox, labels_cpu, annotation_ids) in enumerate(tqdm(dataloader, desc=\"Test Batches\", leave=True)):\n",
    "#         black_images = 0\n",
    "#         context_images = context_images.to(device)\n",
    "#         target_images = target_images.to(device)\n",
    "#         bbox = bbox.to(device)\n",
    "#         labels = labels_cpu.to(device) # keep a copy of labels on cpu to avoid unnecessary transfer back to cpu later\n",
    "#         if simple_model:\n",
    "#             output = model(target_images)\n",
    "#         else:\n",
    "#             output = model(context_images, target_images, bbox) \n",
    "#         _, predictions = torch.max(output.detach(), 1) # choose idx with maximum score as prediction\n",
    "# #         morphed_labels = morph_labels(labels)\n",
    "# #         morphed_predictions = morph_labels(predictions)\n",
    "# #         match = morphed_predictions == morphed_labels\n",
    "#         match = predictions == labels\n",
    "#         all_matches.extend(match)\n",
    "#         totals += len(match)\n",
    "#         corrects += torch.sum(match).item()\n",
    "#         for i in range(len(match)):\n",
    "#             if not match[i].item():\n",
    "#                 test = unorm(target_images[i])\n",
    "#                 if torch.sum(test).item() < 1000:\n",
    "#                     black_images += 1\n",
    "#         totals = totals - black_images\n",
    "# print(corrects/totals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "architectures = ['resnet', 'vit', 'mobilenet', 'squeezenet', 'densenet']\n",
    "transformations = ['light','viewpoint','material']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "perfs = {}\n",
    "for arch in architectures:\n",
    "    perfs[arch] = {}\n",
    "    for trans in transformations:\n",
    "        fol = 'v3_output/%s_%s_transfer_scratch'%(arch, trans)\n",
    "        file = os.path.join(fol, 'scannet_handmade_accuracies.json')\n",
    "        with open(file, 'r') as F:\n",
    "            contents = json.load(F)\n",
    "            acc = \"%0.02f\"%contents['total_accuracy']\n",
    "            perfs[arch][trans] = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'resnet': {'light': '0.18', 'viewpoint': '0.17', 'material': '0.07'},\n",
       " 'vit': {'light': '0.13', 'viewpoint': '0.08', 'material': '0.08'},\n",
       " 'mobilenet': {'light': '0.22', 'viewpoint': '0.14', 'material': '0.22'},\n",
       " 'squeezenet': {'light': '0.16', 'viewpoint': '0.15', 'material': '0.22'},\n",
       " 'densenet': {'light': '0.25', 'viewpoint': '0.14', 'material': '0.23'}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perfs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "domain_adaptation",
   "language": "python",
   "name": "domain_adaptation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
